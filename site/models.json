{
    "vae": {
        "label": "VAE y Modelos de Flujo",
        "models": [
            {
                "name": "MusicVAE",
                "year": 2018,
                "description": "VAE jerárquico para interpolación y generación de secuencias MIDI",
                "paper": {
                    "url": "https://arxiv.org/abs/1803.05428",
                    "cite": "Roberts et al. (2018) — A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music"
                }
            },
            {
                "name": "VQ-VAE / VQ-VAE-2",
                "year": 2017,
                "description": "Codificación discreta de audio en espacio latente, base de Jukebox",
                "paper": {
                    "url": "https://arxiv.org/abs/1711.00937",
                    "cite": "van den Oord et al. (2017) — Neural Discrete Representation Learning (VQ-VAE)"
                }
            },
            {
                "name": "WaveGlow",
                "year": 2019,
                "description": "Flow generativo para síntesis de voz paralela en tiempo real",
                "paper": {
                    "url": "https://arxiv.org/abs/1811.00002",
                    "cite": "Prenger et al. (2019) — WaveGlow: A Flow-based Generative Network for Speech Synthesis"
                }
            },
            {
                "name": "FloWaveNet",
                "year": 2019,
                "description": "Flow-based model para generación paralela de waveforms",
                "paper": {
                    "url": null,
                    "cite": null
                }
            }
        ],
        "examples": [
            {
                "title": "MusicVAE — Melodía 16 compases",
                "model": "MusicVAE",
                "year": 2018,
                "url": "https://magenta.withgoogle.com/assets/music_vae/mel_16bar-a.mp3",
                "context": "Generación de melodías largas desde espacio latente"
            },
            {
                "title": "MusicVAE — Interpolación de batería",
                "model": "MusicVAE",
                "year": 2018,
                "url": "https://magenta.withgoogle.com/assets/music_vae/mel_2bar-dumsamp.mp3",
                "context": "Interpolación en espacio latente entre patrones rítmicos"
            },
            {
                "title": "MusicVAE — Atributo high",
                "model": "MusicVAE",
                "year": 2018,
                "url": "https://magenta.withgoogle.com/assets/music_vae/attr-hi.mp3",
                "context": "Manipulación de atributos musicales en el espacio latente"
            }
        ]
    },
    "gan": {
        "label": "GAN (Generative Adversarial Networks)",
        "models": [
            {
                "name": "GANSynth",
                "year": 2019,
                "description": "GAN para síntesis de notas individuales con control de pitch y timbre",
                "paper": {
                    "url": "https://arxiv.org/abs/1902.08710",
                    "cite": "Engel et al. (2019) — GANSynth: Adversarial Neural Audio Synthesis"
                }
            },
            {
                "name": "WaveGAN",
                "year": 2019,
                "description": "GAN que genera waveforms crudas de un segundo",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "MelGAN",
                "year": 2019,
                "description": "Vocoder GAN rápido para síntesis mel-spectrogram → audio",
                "paper": {
                    "url": "https://arxiv.org/abs/1910.06711",
                    "cite": "Kumar et al. (2019) — MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis"
                }
            },
            {
                "name": "HiFi-GAN",
                "year": 2020,
                "description": "Vocoder de alta fidelidad usado en pipelines TTS y música",
                "paper": {
                    "url": "https://arxiv.org/abs/2010.05646",
                    "cite": "Kong et al. (2020) — HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis"
                }
            }
        ],
        "examples": [
            {
                "title": "GANSynth — Bach con un solo vector latente",
                "model": "GANSynth",
                "year": 2019,
                "url": "https://magenta.withgoogle.com/assets/gansynth/bach_single_latent_vector.mp3",
                "context": "Bach renderizado con timbre constante desde un punto del espacio latente"
            },
            {
                "title": "GANSynth — Bach interpolado",
                "model": "GANSynth",
                "year": 2019,
                "url": "https://magenta.withgoogle.com/assets/gansynth/bach_interpolated.mp3",
                "context": "Interpolación continua de timbre durante una pieza de Bach"
            }
        ]
    },
    "ddsp": {
        "label": "Síntesis Neural / Modelos Físicos Diferenciables",
        "models": [
            {
                "name": "DDSP",
                "year": 2020,
                "description": "Síntesis diferenciable: harmónico + ruido + filtros con control explícito de f0 y loudness",
                "paper": {
                    "url": "https://arxiv.org/abs/2001.04643",
                    "cite": "Engel et al. (2020) — DDSP: Differentiable Digital Signal Processing"
                }
            }
        ],
        "examples": [
            {
                "title": "DDSP — Voz cantada transformada a violín",
                "model": "DDSP",
                "year": 2020,
                "url": "https://storage.googleapis.com/ddsp/timbre_transfer/singing.mp3",
                "context": "Transferencia de timbre: VOZ a violín"
            },
            {
                "title": "DDSP — Violín original",
                "model": "DDSP",
                "year": 2020,
                "url": "https://storage.googleapis.com/ddsp/timbre_transfer/violin.mp3",
                "context": "Transferencia de timbre: voz a VIOLIN"
            }
        ]
    },
    "ar": {
        "label": "Autoregresivo / Token-Based",
        "models": [
            {
                "name": "SampleRNN",
                "year": 2017,
                "description": "RNN multi-escala que genera audio crudo a tres resoluciones temporales",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "WaveNet",
                "year": 2016,
                "description": "Red convolucional causal dilated que genera audio sample a sample",
                "paper": {
                    "url": "https://arxiv.org/abs/1609.03499",
                    "cite": "van den Oord et al. (2016) — WaveNet: A Generative Model for Raw Audio"
                }
            },
            {
                "name": "WaveRNN",
                "year": 2018,
                "description": "RNN eficiente para síntesis de audio en tiempo real (vocoder AR)",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "Music Transformer",
                "year": 2018,
                "description": "Transformer con relative attention para generación de piano largo",
                "paper": {
                    "url": "https://arxiv.org/abs/1809.04281",
                    "cite": "Huang et al. (2018) — Music Transformer: Generating Music with Long-Term Structure"
                }
            },
            {
                "name": "Jukebox",
                "year": 2020,
                "description": "VQ-VAE + Transformer para generar audio con voz en crudo",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "AudioLM",
                "year": 2022,
                "description": "Modelado de lenguaje sobre tokens de audio (SoundStream + w2v-BERT)",
                "paper": {
                    "url": "https://arxiv.org/abs/2209.03143",
                    "cite": "Borsos et al. (2022) — AudioLM: a Language Modeling Approach to Audio Generation"
                }
            },
            {
                "name": "MusicLM",
                "year": 2023,
                "description": "Generación de música a partir de texto, construido sobre AudioLM",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "MusicGen",
                "year": 2023,
                "description": "Transformer de Meta para generación condicionada a texto/melodía",
                "paper": {
                    "url": "https://arxiv.org/abs/2306.05284",
                    "cite": "Copet et al. (2023) — Simple and Controllable Music Generation (MusicGen)"
                }
            }
        ],
        "examples": [
            {
                "title": "Performance RNN — Pieza larga",
                "model": "Performance RNN",
                "year": 2016,
                "url": "https://magenta.withgoogle.com/assets/performance_rnn/long.mp3",
                "context": "RNN generando piano con dinámica y tempo expresivo"
            },
            {
                "title": "Music Transformer — Jazz coherente",
                "model": "Music Transformer",
                "year": 2018,
                "url": "https://magenta.withgoogle.com/assets/music_transformer/relatively_jazz.mp3",
                "context": "Generación autónoma estilo jazz con relative attention"
            },
            {
                "title": "Music Transformer — Continuación de Chopin",
                "model": "Music Transformer",
                "year": 2018,
                "url": "https://magenta.withgoogle.com/assets/music_transformer/primed_chopin_low_repetition.mp3",
                "context": "Primed con fragmento de Chopin, baja repetición"
            },
            {
                "title": "Music Transformer — Visualización de motivos",
                "model": "Music Transformer",
                "year": 2018,
                "url": "https://magenta.withgoogle.com/assets/music_transformer/motifs_visualization.mp4",
                "type": "video",
                "context": "Visualización de motivos melódicos y su recurrencia"
            },
            {
                "title": "MusicGen — Classic reggae con solo de guitarra",
                "model": "MusicGen",
                "year": 2023,
                "url": "https://dl.fbaipublicfiles.com/audiocraft/webpage/public/assets/audios/musicgen_stereo/sample_002.mp3",
                "context": "Generación condicionada a texto: classic reggae track with electronic guitar solo"
            },
            {
                "title": "MusicLM — Electro-reggaetón espacial",
                "model": "MusicLM",
                "year": 2023,
                "url": "https://google-research.github.io/seanet/musiclm/examples/audio_samples/rich-descriptions/electreggaeton/audio.wav",
                "context": "Fusion reggaeton + EDM, prompt: spacey otherworldly sound"
            },
            {
                "title": "Jukebox — Classic pop estilo Elvis Presley",
                "model": "Jukebox",
                "year": 2020,
                "url": "https://soundcloud.com/openai_audio/classic-pop-in-the-style-of-elvis-presley",
                "type": "soundcloud",
                "context": "Audio crudo con voz generado por VQ-VAE + Transformer jerárquico"
            },
            {
                "title": "MuseNet — Pink Panther Remix",
                "model": "MuseNet",
                "year": 2019,
                "url": "https://soundcloud.com/xane-myers/musenet-the-pink-panther-remix",
                "type": "soundcloud",
                "context": "Transformer generando continuación MIDI multi-instrumento"
            }
        ]
    },
    "diffusion": {
        "label": "Difusión / Score-Based",
        "models": [
            {
                "name": "DiffWave",
                "year": 2021,
                "description": "Modelo de difusión para generación de audio waveform",
                "paper": {
                    "url": "https://arxiv.org/abs/2009.09761",
                    "cite": "Kong et al. (2021) — DiffWave: A Versatile Diffusion Model for Audio Synthesis"
                }
            },
            {
                "name": "WaveGrad",
                "year": 2021,
                "description": "Difusión condicional para síntesis de voz de alta calidad",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "Riffusion",
                "year": 2022,
                "description": "Stable Diffusion fine-tuneado sobre espectrogramas para generar música",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "AudioLDM2",
                "year": 2023,
                "description": "Latent diffusion multi-modal para audio, música y voz",
                "paper": {
                    "url": "https://arxiv.org/abs/2308.05734",
                    "cite": "Liu et al. (2023) — AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining"
                }
            },
            {
                "name": "Stable Audio Open",
                "year": 2024,
                "description": "Modelo abierto de Stability AI para generación de audio y música",
                "paper": {
                    "url": "https://arxiv.org/abs/2407.14358",
                    "cite": "Evans et al. (2024) — Stable Audio Open"
                }
            },
            {
                "name": "Mustango",
                "year": 2023,
                "description": "Text-to-music con control musical explícito (tempo, acorde, instrumento)",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "MusicLDM",
                "year": 2023,
                "description": "Latent diffusion específico para generación de música",
                "paper": {
                    "url": null,
                    "cite": null
                }
            }
        ],
        "examples": [
            {
                "title": "Riffusion — Bossa nova con guitarra eléctrica",
                "model": "Riffusion",
                "year": 2022,
                "url": "https://upload.wikimedia.org/wikipedia/commons/0/0a/AI-generated_audio_featuring_bossa_nova_music_with_electric_guitar.ogg",
                "context": "Generado desde prompt de texto vía difusión sobre espectrogramas"
            },
            {
                "title": "AudioLDM2 — Violín con melodía emotiva",
                "model": "AudioLDM2",
                "year": 2023,
                "url": "https://audioldm.github.io/audioldm2/demos/ttm_headline_nine_audio/A_violin_playing_a_heartfelt_melody.flac",
                "context": "Prompt: A violin playing a heartfelt melody"
            },
            {
                "title": "AudioLDM2 — Overview del modelo",
                "model": "AudioLDM2",
                "year": 2023,
                "url": "https://audioldm.github.io/audioldm2/Overview.mp4",
                "type": "video",
                "context": "Demostración general de generación de audio, música y voz"
            }
        ]
    },
    "tokenizers": {
        "label": "Tokenizers (Audio Codecs)",
        "models": [
            {
                "name": "SoundStream",
                "year": 2021,
                "description": "Primer codec neural end-to-end con RVQ, base de AudioLM",
                "paper": {
                    "url": "https://arxiv.org/abs/2107.03312",
                    "cite": "Zeghidour et al. (2021) — SoundStream: An End-to-End Neural Audio Codec"
                }
            },
            {
                "name": "EnCodec",
                "year": 2022,
                "description": "Codec neural de Meta con RVQ, usado en MusicGen y AudioLM",
                "paper": {
                    "url": "https://arxiv.org/abs/2210.13438",
                    "cite": "Défossez et al. (2022) — High Fidelity Neural Audio Compression (EnCodec)"
                }
            },
            {
                "name": "DAC (Descript Audio Codec)",
                "year": 2023,
                "description": "Codec de alta fidelidad optimizado para música con menor bitrate",
                "paper": {
                    "url": "https://arxiv.org/abs/2306.06546",
                    "cite": "Kumar et al. (2023) — High-Fidelity Audio Compression with Improved RVQGAN (DAC)"
                }
            }
        ],
        "examples": []
    },
    "supervised": {
        "label": "Supervisado (Datos con Etiquetas)",
        "models": [
            {
                "name": "YAMNet",
                "year": 2020,
                "description": "Clasificador de 521 eventos sonoros sobre MobileNet, entrenado en AudioSet",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "VGGish",
                "year": 2017,
                "description": "Embeddings de audio basados en VGG16 entrenado en AudioSet",
                "paper": {
                    "url": null,
                    "cite": null
                }
            }
        ],
        "examples": []
    },
    "selfsupervised": {
        "label": "Auto Supervisado (Masking, Asociativo, Contrastivo)",
        "models": [
            {
                "name": "SSAST",
                "year": 2022,
                "description": "Self-Supervised Audio Spectrogram Transformer con masked patching",
                "paper": {
                    "url": "https://arxiv.org/abs/2110.09784",
                    "cite": "Gong et al. (2022) — SSAST: Self-Supervised Audio Spectrogram Transformer"
                }
            },
            {
                "name": "BYOL-A",
                "year": 2022,
                "description": "Bootstrap Your Own Latent for Audio",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "MERT",
                "year": 2023,
                "description": "Music understanding model pre-entrenado con teacher acústico + musical",
                "paper": {
                    "url": "https://arxiv.org/abs/2306.00107",
                    "cite": "Li et al. (2023) — MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training"
                }
            },
            {
                "name": "MusicFM",
                "year": 2024,
                "description": "Foundation model para música con masked prediction multi-tarea",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "EncodecMAE",
                "year": 2024,
                "description": "Masked autoencoder sobre tokens de Encodec para representaciones de audio",
                "paper": {
                    "url": null,
                    "cite": null
                }
            },
            {
                "name": "MuQ",
                "year": 2024,
                "description": "Music understanding quantizer con pre-training multi-tarea",
                "paper": {
                    "url": "https://arxiv.org/abs/2505.16306",
                    "cite": "MuQ + layer-wise analysis (2025) — arXiv:2505.16306"
                }
            },
            {
                "name": "SoniDo",
                "year": 2024,
                "description": "Modelo auto-supervisado para separación y análisis de fuentes musicales",
                "paper": {
                    "url": null,
                    "cite": null
                }
            }
        ],
        "examples": []
    },
    "multimodal": {
        "label": "Modelado Multimodal",
        "models": [
            {
                "name": "CLAP",
                "year": 2023,
                "description": "Contrastive Language-Audio Pretraining — alinea audio y texto en un espacio compartido",
                "paper": {
                    "url": "https://arxiv.org/abs/2211.06687",
                    "cite": "Elizalde et al. (2023) — CLAP: Learning Audio Concepts From Natural Language Supervision"
                }
            },
            {
                "name": "OpenL3",
                "year": 2019,
                "description": "Look, Listen, and Learn — Modelo de representación de audio y video basado en aprendizaje profundo",
                "paper": {
                    "url": null,
                    "cite": null
                }
            }
        ],
        "examples": []
    },
    "representations": {
        "label": "Análisis de Representaciones",
        "models": [],
        "examples": []
    },
    "embeddings": {
        "label": "Generación de Espacio Vectorial (Embeddings)",
        "models": [],
        "examples": []
    }
}